# Spanish legal domain Language Model ‚öñÔ∏è
This repository contains the page for two main resources for the Spanish legal domain:
- A RoBERTa model: https://huggingface.co/PlanTL-GOB-ES/RoBERTalex
- FastText embeddings: https://zenodo.org/record/5036147
- Legal corpora: https://zenodo.org/record/5495529#.YXFlr577SUk

The repository and the pre-print will be updated with larger models, evaluations, ...

## Why‚ùì
There are two main models made specifically for the Spanish language, the [BETO model](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) and a [GPT-2](https://huggingface.co/datificate/gpt2-small-spanish).
There is also a multilingual BERT (mBERT) that is often used as it might be better sometimes.

Both BETO and GPT-2 models for Spanish have been trained with rather low resources, 4GB and 3GB of data respectively.
The data used for training both models might be various but the amount is not enough to cover all domains.
Furthermore, training a BERT-like domain-specific model is better as it effectively covers the vocabulary and understands the legal jargon.
We present our models trained on 9GB that are specifically of the legal domain.

## Evaluation ‚úÖ
_Work in progress._

## Corpora üìÉ
| Corpus name                                   | Size (GB) | Tokens (M) |
|-----------------------------------------------|-----------|------------|
| Procesos Penales                              |     0.625 |      0.119 |
| JRC Acquis                                    |     0.345 |     59.359 |
| C√≥digos Electr√≥nicos Universitarios           |     0.077 |     11.835 |
| C√≥digos Electr√≥nicos                          |     0.080 |     12.237 |
| Doctrina de la Fiscal√≠a General del Estado    |     0.017 |      2.669 |
| Legislaci√≥n BOE                               |     3.600 |    578.685 |
| Abogac√≠a del Estado BOE                       |     0.037 |      6.123 |
| Consejo de Estado: Dict√°menes                 |     0.827 |    135.348 |
| Spanish EURLEX                                |     0.001 |      0.072 |
| UN Resolutions                                |     0.023 |      3.539 |
| Spanish DOGC                                  |     0.826 |    132.569 |
| Spanish MultiUN                               |     2.200 |    352.653 |
| Consultas Tributarias Generales y Vinculantes |     0.466 |     77.691 |
| Constituci√≥n Espa√±ola                         |     0.002 |      0.018 |
| COPPA Patents Corpus                          |     0.002 |          - |
| Biomedical Patents                            |     0.083 |          - |


## Usage example ‚öóÔ∏è
You can train your model for different downstream tasks using the scripts that Hugging Face provides ([Name Entity Recognition](https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py), [GLUE tasks](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py) and [others](https://github.com/huggingface/transformers/tree/master/examples/pytorch))

```python
print("TBA")
```

## Cite üì£
If this work is helpful, please cite it:
```
TBA
```

## Contact üìß
üìã We are interested in (1) extending our corpora to make larger models (2) evaluate/train the model in other tasks.

For questions regarding this work, contact Asier Guti√©rrez-Fandi√±o (<asier.gutierrez@bsc.es>)
